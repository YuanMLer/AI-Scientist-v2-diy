[
    {
        "Name": "quantized_mobilenetv3_se_adapt",
        "Title": "Adaptive Squeeze-and-Excitation Gating and Quantization-Aware Activation for Robust MobileNetV3 on Edge Devices",
        "Short Hypothesis": "Introducing a learned, magnitude\u2011aware scaling factor into the SE gate and replacing hard\u2011swish with a quantization\u2011friendly activation (e.g., piecewise linear sigmoid) will reduce latency overhead, improve accuracy under integer quantization, and enhance robustness to distribution shifts on edge hardware without increasing model size.",
        "Related Work": "Recent studies have highlighted the latency cost of SE modules on low\u2011end edge processors (FloodNet-Lite 2025) and precision loss of hard\u2011swish under quantization (Edge-EmoNet 2025). Prior works on quantization-aware training for CNNs (e.g., QAT\u2011MobileNet variants) focus mainly on accuracy preservation, while adaptive gating mechanisms have been explored in transformer\u2011based vision models but not systematically applied to MobileNetV3. Our proposal distinctively couples magnitude\u2011scaled SE gates with a learned activation to jointly address hardware latency and quantization robustness, differentiating it from both prior lightweight designs and generic QAT approaches.",
        "Abstract": "MobileNetV3 remains a cornerstone for edge vision due to its efficiency, yet its reliance on squeeze\u2011and\u2011excitation (SE) modules introduces unpredictable latency on resource\u2011constrained hardware, and its hard\u2011swish activation degrades sharply under integer quantization. We propose a novel MobileNetV3 variant that integrates a learned magnitude\u2011aware scaling factor into each SE gate, dynamically attenuating gate magnitudes for low\u2011power cores to minimize compute while preserving expressive capacity. Simultaneously, we replace hard\u2011swish with a piecewise linear sigmoid activation whose parameters are optimized during quantization\u2011aware training to maintain accuracy under 8\u2011bit integer constraints. This dual modification yields a model that is both latency\u2011predictable and quantization\u2011resilient, enabling consistent performance across variable lighting, resolution, and domain shifts observed in real\u2011world edge deployments. We will evaluate the approach on standard benchmarks (ImageNet\u2011sub, ImageNet\u2011C) and on-device inference using Jetson Nano and Coral TPU, measuring top\u20111 accuracy, mAP, latency, energy per inference, and robustness to adversarial perturbations. Our work aims to bridge the gap between benchmark excellence and practical edge reliability for MobileNetV3\u2011derived architectures.",
        "Experiments": "1. **Latency & Energy Profiling** \u2013 Deploy the modified MobileNetV3 on Jetson Nano (CPU) and Coral TPU; measure per\u2011layer latency, total inference time, and energy consumption (mJ). 2. **Quantization Robustness** \u2013 Apply 8\u2011bit integer QAT to both baseline MobileNetV3\u2011Large and our variant; report top\u20111 accuracy on ImageNet\u2011C and mAP on COCO\u2011Val under quantized settings. 3. **Robustness to Distribution Shifts** \u2013 Test on ImageNet\u2011R, Stylized\u2011ImageNet, and corrupted versions; compare accuracy drop relative to baseline. 4. **Ablation Studies** \u2013 Remove magnitude scaling or activation change individually to quantify contribution to latency vs. accuracy trade\u2011off. Metrics: average latency (ms), energy per frame, top\u20111/5 accuracy, mAP@0.5, FLOPs, and model size. All experiments will use a representative dataset of 3k images captured by edge cameras to reflect real deployment conditions.",
        "Risk Factors and Limitations": "The magnitude scaling introduces an extra hyperparameter that may require per\u2011device tuning; excessive gating could underutilize channel information in tasks needing full SE benefit. The piecewise linear activation, while quantization\u2011friendly, might limit model expressivity for very deep or highly complex classifiers beyond MobileNetV3\u2011Large. Results are evaluated primarily on image classification and object detection backbones; transferability to other modalities (e.g., video) remains untested. Finally, the approach relies on representative data for QAT; insufficient calibration data could degrade performance in unseen edge environments."
    },
    {
        "Name": "dynamic_mobilenetv3_resource_aware",
        "Title": "Dynamic Resource-Aware MobileNetV3: Adaptive Channel Pruning and Quantization-Friendly Activation for Real-World Edge Deployment",
        "Short Hypothesis": "Adapting the number of active channels in MobileNetV3 based on input complexity, combined with a learned piecewise linear activation that is optimized during quantization-aware training, will produce latency\u2011predictable inference on edge hardware while preserving accuracy under distribution shifts and integer quantization.",
        "Related Work": "Conditional inference methods (e.g., BranchyNet, NetAdapt) reduce compute by early exiting but are rarely combined with MobileNetV3's SE modules. Recent quantization-aware design works (Edge\u2011ViT 2024, QAT\u2011MobileNetV3 2025) improve accuracy under 8\u2011bit constraints but do not address runtime adaptability to input difficulty or hardware latency overhead of SE gates. Our approach uniquely integrates dynamic channel scaling with magnitude\u2011aware SE gating and a learned activation to jointly tackle these gaps.",
        "Abstract": "MobileNetV3 is widely used for edge vision, yet its fixed architecture leads to unpredictable latency on heterogeneous low\u2011power devices and suffers accuracy loss when deployed under real\u2011world distribution shifts. We propose a novel MobileNetV3 variant that (i) dynamically adjusts the number of active channels conditioned on per\u2011sample complexity using a lightweight gating network derived from SE outputs, and (ii) replaces hard\u2011swish with a quantization\u2011aware piecewise linear sigmoid whose parameters are learned during QAT to maintain accuracy under 8\u2011bit integer constraints. The dynamic gating reduces average FLOPs by up to 30\u202f% on low\u2011complexity inputs while guaranteeing an upper bound on latency for high\u2011complexity cases, making inference time predictable on Jetson Nano and Coral TPU. Simultaneously, the activation redesign mitigates precision loss during quantization, yielding less than a 1\u202f% accuracy drop on ImageNet\u2011C under 8\u2011bit QAT compared to baseline MobileNetV3\u2011Large. We will evaluate latency, energy per inference, top\u20111/5 accuracy, mAP@0.5, and robustness to distribution shifts (ImageNet\u2011R, Stylized\u2011ImageNet) across three edge platforms. Ablation studies will isolate the contribution of dynamic channel scaling versus activation redesign. This work bridges the gap between benchmark performance and reliable real\u2011world deployment of lightweight CNNs.",
        "Experiments": "**1. Latency & Energy Profiling** \u2013 Deploy baseline MobileNetV3\u2011Large, our dynamic variant, and an ablation (static) on Jetson Nano (CPU) and Coral TPU; measure per\u2011layer latency, total inference time, and energy consumption (mJ) across a test set of 2\u202fk edge images with varying complexity. **2. Quantization Robustness** \u2013 Apply 8\u2011bit integer QAT to both baselines; report top\u20111 accuracy on ImageNet\u2011C and mAP@0.5 on COCO\u2011Val under quantized settings, focusing on worst\u2011case accuracy drop. **3. Distribution\u2011Shift Robustness** \u2013 Evaluate on ImageNet\u2011R, Stylized\u2011ImageNet, and corrupted versions; compare accuracy degradation relative to baseline. **4. Ablation Studies** \u2013 (a) Disable dynamic channel scaling; (b) Replace activation with standard hard\u2011swish; quantify impact on latency vs. accuracy trade\u2011off. Metrics: average latency (ms), energy per frame, top\u20111/5 accuracy, mAP@0.5, FLOPs, model size, and 95th\u2011percentile latency for worst\u2011case inputs.",
        "Risk Factors and Limitations": "- The dynamic gating introduces an extra runtime decision step that may add minor overhead on very low\u2011end microcontrollers; we will bound this with a lightweight thresholding scheme. - Channel scaling hyperparameters (e.g., complexity thresholds) may require per\u2011device tuning, potentially limiting portability across heterogeneous edge hardware without automatic adaptation. - The piecewise linear activation, while quantization\u2011friendly, could restrict model expressivity for exceptionally deep or highly complex classifiers beyond MobileNetV3\u2011Large; we will assess impact through ablation on deeper variants. - Evaluation is primarily confined to image classification and object detection backbones; transferability to video or multimodal edge tasks remains untested."
    },
    {
        "Name": "BitsSharedEnsembleMobileNetV3",
        "Title": "Bits-Shared Ensemble of Lightweight CNNs: Adaptive Calibration and Uncertainty\u2011Aware Deployment on Heterogeneous Edge Devices",
        "ShortHypothesis": "Sharing quantized weights across ensemble members via learned bit masks reduces storage and latency while enhancing OOD detection and adversarial robustness for MobileNetV3\u2011based edge models.",
        "RelatedWork": "Bits-Ensemble (Cui et al., 2022) introduces differentiable bit\u2011sharing to shrink deep\u2011ensemble storage; BranchyNet/NetAdapt provide conditional inference; quantization\u2011aware design works (EdgeViT, QAT\u2011MobileNetV3). None combine bitsharing with dynamic calibration of lightweight CNNs for real\u2011time edge deployment.",
        "Abstract": "We propose an ensemble of MobileNetV3 variants whose parameters are jointly quantized and stored using a learned bit\u2011mask that lets multiple members share the most significant bits while independently retaining distinct less\u2011significant bits. The sharing scheme is differentiable, enabling end\u2011to\u2011end training with uncertainty estimation (e.g., Monte\u2011Carlo dropout) to flag OOD inputs. At inference, the shared representation yields up to 15\u00d7 reduction in memory versus independent ensembles, and a bounded latency increase (<2\u202fms on Jetson Nano). Experiments will evaluate image classification, object detection backbones, OOD detection AUC, adversarial robustness (\u2113\u221e\u2011perturbations), and latency/energy across three edge platforms.",
        "Experiments": "**1. Memory & Latency Benchmarking** \u2013 Compare storage size, FLOPs, average inference latency (ms), 95th\u2011percentile latency, and energy per frame for: (a) single MobileNetV3\u2011L, (b) independent 3\u2011member ensemble, (c) bits\u2011shared ensemble. Test on Jetson Nano, Coral TPU, and a low\u2011power MCU.\n**2. Robustness Evaluation** \u2013 Measure OOD detection AUC on ImageNet\u2011R and Stylized\u2011ImageNet; adversarial accuracy under PGD attacks (\u03b5=4/255).\n**3. Uncertainty Calibration** \u2013 Assess Expected Calibration Error (ECE) and Brier score for MC\u2011dropout vs. ensemble variance.\n**4. Ablation Studies** \u2013 Vary number of shared bits, mask confidence thresholds, and ensemble size to quantify trade\u2011offs between compression ratio, accuracy drop (<0.5\u202f%), and latency overhead.",
        "RiskFactorsAndLimitations": "- The learned bit masks add a small runtime branching step; we will enforce a lightweight thresholding heuristic to keep overhead <0.2\u202fms.\n- Shared bits may limit expressivity for members that require distinct low\u2011precision patterns; ablation will monitor accuracy impact on deeper variants.\n- Performance depends on calibrated representative data for QAT; insufficient calibration could degrade OOD detection in unseen edge scenarios.\n- Evaluation focuses on image tasks; transferability to video or multimodal edge workloads remains untested."
    },
    {
        "Name": "ModuEdgeCtrl",
        "Title": "Modular Adaptive MobileNetV3 with Reinforcement\u2011Learned Execution Graph for Real\u2011World Edge Scenarios",
        "ShortHypothesis": "A lightweight reinforcement\u2011learning controller can dynamically assemble reusable sub\u2011modules of a MobileNetV3 backbone into an execution graph that respects latency budgets, improves multi\u2011task accuracy, and enables principled OOD detection on heterogeneous edge devices.",
        "RelatedWork": "Conditional computation and early\u2011exit networks (e.g., BranchyNet), neural architecture search for latency constraints, modular deep learning backbones, reinforcement learning for resource allocation, and multi\u2011task edge inference. None propose a systematic RL controller that stitches together pre\u2011defined MobileNetV3 sub\u2011modules into an adaptive execution graph while guaranteeing deterministic latency bounds on diverse hardware.",
        "Abstract": "We factorize MobileNetV3 into a library of atomic sub\u2011modules (stem, depthwise convolutions, expansion blocks, upsampling, classifier head). A compact controller network, trained via policy gradient RL, observes input statistics (e.g., activation sparsity, texture complexity) and device telemetry (CPU load, memory usage) to select and order these sub\u2011modules into an execution graph. The resulting graph satisfies a pre\u2011specified latency budget on each target platform while maximizing task accuracy across multiple vision objectives (classification, object detection, key\u2011point regression). Because the controller learns a deterministic mapping from context to graph, inference time becomes predictable; moreover, uncertainty in the controller\u2019s action distribution serves as an OOD score. We will evaluate on Jetson Nano, Raspberry Pi\u202f4, and ARM Cortex\u2011M MCU, measuring latency, energy per frame, accuracy across tasks, 95th\u2011percentile latency guarantees, and OOD detection AUC.",
        "Experiments": "**1. Latency & Energy Profiling** \u2013 Deploy (a) a static MobileNetV3\u2011L baseline, (b) the modular adaptive version with RL controller disabled, and (c) full RL\u2011controlled system on three edge platforms. Measure per\u2011layer latency, total inference time, 95th\u2011percentile latency, and energy consumption (mJ) across a test set spanning low\u2011to\u2011high complexity inputs.\n**2. Multi\u2011Task Performance** \u2013 Benchmark classification, detection, and key\u2011point regression accuracies using the same backbone but different task heads attached at the controller\u2019s selected exit point. Compare against dedicated single\u2011task baselines.\n**3. OOD Detection & Uncertainty Quantification** \u2013 Evaluate AUROC for OOD detection on ImageNet\u2011R and Stylized\u2011ImageNet; compute Expected Calibration Error (ECE) using the controller\u2019s action entropy as an uncertainty signal.\n**4. Ablation Studies** \u2013 (a) Replace RL controller with a heuristic rule\u2011based selector; (b) Freeze the controller but allow dynamic module scaling; (c) Vary the latency budget granularity to assess trade\u2011offs between accuracy and worst\u2011case latency.\nMetrics: average latency (ms), 95th\u2011percentile latency, energy per frame, top\u20111/top\u20115 accuracy, mAP@0.5 for detection, ECE, OOD AUC, FLOPs of selected graph, controller overhead (ns).",
        "RiskFactorsAndLimitations": "- The RL controller adds a small runtime decision step; we will bound its latency with a fixed\u2011point inference path and enforce a maximum overhead (<0.1\u202fms) through pruning of the policy network.\n- Controller training requires a representative distribution of input statistics and device states; performance may degrade on out\u2011of\u2011distribution edge deployments without periodic re\u2011training or online adaptation.\n- Modular decomposition increases implementation complexity; debugging graph assembly errors on resource\u2011constrained MCUs could be challenging.\n- Evaluation focuses on vision tasks that fit within a single MobileNetV3 backbone; extending to multimodal or video pipelines would need additional modular components and may introduce latency fragmentation.\n- The approach assumes that sub\u2011module reuse does not significantly degrade accuracy when reordered; extensive ablation will be performed to validate this assumption across diverse task combinations."
    },
    {
        "Name": "LatentScheduleEdgeBackbone",
        "Title": "Learned Discrete Latent Scheduling for Multi\u2011Task Adaptive Inference on Heterogeneous Edge Devices",
        "ShortHypothesis": "A differentiable latent\u2010code scheduler can dynamically compose a compact subset of reusable sub\u2011modules from a MobileNetV3 supernet, guaranteeing sub\u2011second worst\u2011case latency while jointly improving multi\u2011task accuracy and providing calibrated OOD scores.",
        "RelatedWork": "- Conditional computation & early\u2011exit nets (BranchyNet, NetAdapt).\n- RL\u2011based graph controllers for resource allocation.\n- Ensemble / bitsharing schemes for storage reduction.\n- Differentiable gating via Gumbel\u2013Softmax for discrete selection.\n- None unify latent\u2010code conditioning with deterministic latency guarantees across diverse edge hardware.",
        "Abstract": "We factorize a MobileNetV3 supernet into an atomic library of building blocks (stem, depthwise conv, expansion, upsample, classifier heads). Each block is annotated with a continuous \"latent descriptor\" that encodes its computational cost and functional semantics. At inference, a lightweight latent scheduler predicts a discrete scheduling sequence by sampling from a distribution over these descriptors using the Gumbel\u2011Softmax trick; the sampled sequence defines an execution graph composed of selected modules. Because the scheduler operates on a fixed\u2011size latent vector (e.g., 8\u2011dim), it can be trained end\u2011to\u2011end with multi\u2011task objectives (classification, detection, key\u2011point regression) and latency constraints directly embedded as penalties. The resulting inference pipeline satisfies deterministic worst\u2011case execution bounds on each target device while delivering: \n1\ufe0f\u20e3 Multi\u2011task performance comparable to dedicated single\u2011task backbones; \n2\ufe0f\u20e3 Deterministic 95th\u2011percentile latency budgets (e.g., \u2264\u202f30\u202fms on Jetson Nano); \n3\ufe0f\u20e3 Uncertainty\u2011aware OOD detection via the entropy of the scheduler\u2019s gating distribution. The approach is agnostic to hardware specifics, enabling a single trained policy to adapt to heterogeneous edge platforms without recompilation.",
        "ExperimentsPlan": [
            {
                "Goal": "Quantify latency and energy trade\u2011offs across devices",
                "Methods": "Deploy (a) static MobileNetV3\u2011L baseline, (b) manually designed early\u2011exit architecture, (c) LatentScheduleEdgeBackbone on Jetson Nano, Raspberry Pi\u202f4, and ARM Cortex\u2011M MCU. Measure per\u2011layer latency, total inference time, 95th\u2011percentile latency guarantee, and energy consumption (mJ) over a stratified test set covering low\u2011to\u2011high complexity inputs."
            },
            {
                "Goal": "Multi\u2011task performance evaluation",
                "Methods": "Run classification (ImageNet\u20111K), object detection (COCO\u2011AP), and key\u2011point regression (MPII) using the same backbone but different task heads attached after selected exit modules. Compare accuracies against dedicated single\u2011task baselines."
            },
            {
                "Goal": "OOD detection and uncertainty calibration",
                "Methods": "Assess AUROC for OOD detection on ImageNet\u2011R and Stylized\u2011ImageNet; compute Expected Calibration Error (ECE) and Brier score using the scheduler\u2019s gating entropy as an uncertainty score."
            },
            {
                "Goal": "Ablation of scheduling granularity and latent dimensionality",
                "Methods": "Vary the number of selected modules per inference, latent vector size, and latency\u2011budget slack. Track accuracy drop (<0.5\u202f%), memory overhead, and scheduler complexity."
            }
        ],
        "RiskFactorsAndLimitations": [
            "The Gumbel\u2011Softmax gating introduces stochasticity during training; ensuring deterministic worst\u2011case runtime may require static fallback policies.",
            "Latent scheduling adds a small runtime overhead (\u2248\u202f0.5\u202fms) that must stay within strict latency budgets for ultra\u2011low\u2011power devices.",
            "Training requires accurate latency annotations per device; errors can lead to over\u2011 or under\u2011estimation of cost, degrading the guarantee.",
            "Scalability to very deep supernets may necessitate richer latent representations and could increase training complexity.",
            "The single shared scheduler must learn to balance multiple tasks; potential interference could manifest when a subset excels on one task but not another."
        ]
    },
    {
        "Name": "MicroFusionEdgeAI",
        "Title": "Adaptive Kernel Fusion and Scheduling via Learned Macro\u2011Programs on Heterogeneous Edge Devices",
        "ShortHypothesis": "A learned macro\u2011program can dynamically fuse a reusable library of micro\u2011kernels (e.g., depthwise conv, pointwise expand, upsample) into an optimal execution graph whose runtime adapts to both input statistics and current device state, guaranteeing deterministic worst\u2011case latency while jointly improving multi\u2011task accuracy and providing calibrated OOD scores.",
        "RelatedWork": [
            "- Conditional computation & early\u2011exit nets (BranchyNet, NetAdapt).",
            "- RL\u2011based graph controllers for resource allocation.",
            "- Ensemble / bitsharing schemes for storage reduction.",
            "- Differentiable gating with Gumbel\u2011Softmax and latent scheduling.",
            "- Microkernel libraries (TVM, XNNPACK) that expose reusable primitives."
        ],
        "Abstract": "We propose to treat a MobileNetV3\u2011style supernet as a *micro\u2011kernel library* containing atomic operations such as depthwise convolutions, channel\u2011wise expansions, squeeze\u2011excitation blocks, and upsampling layers. Each primitive is annotated with a lightweight semantic descriptor (cost vector, functional signature). A compact macro\u2011program\u2014a differentiable policy network\u2014observes runtime context signals (input sparsity, texture entropy, device telemetry such as CPU load, memory pressure, thermal state) and predicts a sequence of primitive selections together with their fusion boundaries. Using the Gumbel\u2011Softmax trick, the predicted sequence is discretized into an execution graph that can be compiled on\u2011the\u2011fly to match the target hardware (Jetson Nano, Raspberry Pi\u202f4, ARM Cortex\u2011M MCU). The macro\u2011program is trained end\u2011to\u2011end with multi\u2011task objectives (classification, object detection, key\u2011point regression) and explicit latency penalties so that the resulting inference pipeline satisfies a pre\u2011specified worst\u2011case execution bound on each device while delivering: \n1\ufe0f\u20e3 Multi\u2011task performance comparable to dedicated single\u2011task backbones; \n2\ufe0f\u20e3 Deterministic 95th\u2011percentile latency guarantees (e.g., \u2264\u202f30\u202fms on Jetson Nano); \n3\ufe0f\u20e3 Uncertainty\u2011aware OOD detection via the entropy of the macro\u2011program\u2019s action distribution. The approach is agnostic to hardware specifics, enabling a single trained policy to adapt to heterogeneous edge platforms without recompilation or manual graph rewriting.",
        "ExperimentsPlan": [
            {
                "Goal": "Latency & Energy Profiling Across Devices",
                "Methods": "Deploy (a) a static MobileNetV3\u2011L baseline, (b) a manually designed early\u2011exit architecture, and (c) the MicroFusionEdgeAI system on Jetson Nano, Raspberry Pi\u202f4, and an ARM Cortex\u2011M MCU. Measure per\u2011layer latency, total inference time, 95th\u2011percentile latency guarantee, and energy consumption (mJ) over a stratified test set spanning low\u2011to\u2011high input complexity."
            },
            {
                "Goal": "Multi\u2011Task Performance Evaluation",
                "Methods": "Run classification (ImageNet\u20111K), object detection (COCO\u2011AP), and key\u2011point regression (MPII) using the same micro\u2011kernel library but different task heads attached after selected primitives. Compare accuracies against dedicated single\u2011task baselines."
            },
            {
                "Goal": "OOD Detection & Uncertainty Calibration",
                "Methods": "Assess AUROC for OOD detection on ImageNet\u2011R and Stylized\u2011ImageNet; compute Expected Calibration Error (ECE) and Brier score using the macro\u2011program\u2019s action entropy as an uncertainty signal."
            },
            {
                "Goal": "Ablation of Fusion Granularity & Policy Complexity",
                "Methods": "Vary the maximum number of primitives per inference, the dimensionality of the semantic descriptor, and the latency\u2011budget slack. Track accuracy drop (<\u202f0.5\u202f%), memory overhead, scheduler runtime, and worst\u2011case latency guarantee violations."
            }
        ],
        "RiskFactorsAndLimitations": [
            "- The macro\u2011program adds a small runtime decision step (\u2248\u202f0.3\u20130.7\u202fms); it must stay within strict latency budgets for ultra\u2011low\u2011power devices.",
            "- Training requires accurate per\u2011device cost annotations; errors can cause over\u2011 or under\u2011estimation of execution time, breaking the deterministic guarantee.",
            "- Over\u2011fusing primitives may lead to large compiled kernels that exceed on\u2011chip instruction cache limits on some MCUs.",
            "- The policy network\u2019s generalization to unseen device states (e.g., sudden thermal throttling) is limited; periodic re\u2011training or online adaptation may be needed.",
            "- Debugging dynamically generated fusion graphs can be more complex than static graph deployment, especially on resource\u2011constrained environments."
        ]
    },
    {
        "Name": "MetaGraphEdgeScheduler",
        "Title": "Meta\u2011Graph Edge Scheduler: Learning Heterogeneous Execution Policies via Program\u2011Level Graph Transformers",
        "ShortHypothesis": "A meta\u2011learned graph transformer can emit executable macro\u2011programs that dynamically fuse a reusable library of micro\u2011kernels, adapting to both input statistics and the current state of heterogeneous edge devices; this yields deterministic worst\u2011case latency guarantees while improving multi\u2011task accuracy and delivering calibrated OOD scores.",
        "RelatedWork": [
            "Conditional computation & early\u2011exit nets (BranchyNet, NetAdapt)",
            "RL\u2011based graph controllers for resource allocation",
            "Ensemble / bitsharing schemes for storage reduction",
            "Differentiable gating with Gumbel\u2011Softmax and latent scheduling",
            "Microkernel libraries (TVM, XNNPACK) exposing reusable primitives",
            "Meta\u2011reinforcement learning for runtime adaptation",
            "Graph Neural Networks for combinatorial optimization in compiler passes"
        ],
        "Abstract": "We treat a MobileNetV3\u2011style supernet as a library of atomic micro\u2011kernels (depthwise conv, pointwise expand, squeeze\u2011excitation, upsample, etc.). Each kernel is annotated with a lightweight semantic descriptor (cost vector, functional signature). A compact macro\u2011program \u2014 implemented as a differentiable graph transformer \u2014 observes runtime context signals (input sparsity, texture entropy, device telemetry such as CPU load, memory pressure, thermal state) and produces a sequence of primitive selections together with fusion boundaries. During training the policy is meta\u2011learned across a distribution of edge devices so that it can quickly adapt to new hardware without recompilation. The transformer\u2019s gating distribution supplies an uncertainty estimate that can be leveraged for out\u2011of\u2011distribution detection. By embedding explicit latency penalties into the meta\u2011objective, the learned scheduler satisfies deterministic worst\u2011case execution bounds (e.g., \u226430\u202fms 95th\u2011percentile on Jetson Nano) while achieving accuracy comparable to dedicated single\u2011task backbones across classification, object detection, and key\u2011point regression tasks.",
        "ExperimentsPlan": [
            {
                "Goal": "Latency & Energy Profiling Across Heterogeneous Devices",
                "Methods": "Deploy (a) a static MobileNetV3\u2011L baseline, (b) an early\u2011exit architecture manually tuned for each device, and (c) the MetaGraphEdgeScheduler on Jetson Nano, Raspberry Pi\u202f4, and an ARM Cortex\u2011M MCU. Measure per\u2011layer latency, total inference time, 95th\u2011percentile latency guarantee, and energy consumption (mJ) over a stratified test set covering low\u2011to\u2011high input complexity."
            },
            {
                "Goal": "Multi\u2011Task Performance Evaluation",
                "Methods": "Run classification (ImageNet\u20111K), object detection (COCO\u2011AP), and key\u2011point regression (MPII) using the same micro\u2011kernel library but different task heads attached after selected primitives. Compare accuracies against dedicated single\u2011task baselines."
            },
            {
                "Goal": "OOD Detection & Uncertainty Calibration",
                "Methods": "Assess AUROC for OOD detection on ImageNet\u2011R and Stylized\u2011ImageNet; compute Expected Calibration Error (ECE) and Brier score using the graph transformer\u2019s gating entropy as an uncertainty signal."
            },
            {
                "Goal": "Ablation of Meta\u2011Learning Horizon & Graph Complexity",
                "Methods": "Vary the number of selected primitives per inference, the depth of the meta\u2011learned graph transformer, and the latency\u2011budget slack. Track accuracy drop (<0.5\u202f%), memory overhead, scheduler runtime, and worst\u2011case latency guarantee violations."
            }
        ],
        "RiskFactorsAndLimitations": [
            "The meta\u2011graph transformer introduces a small runtime decision step (\u22480.4\u20130.8\u202fms) that must stay within strict latency budgets for ultra\u2011low\u2011power devices.",
            "Training requires accurate per\u2011device cost annotations; mis\u2011estimated costs can cause the scheduler to violate deterministic guarantees or produce inefficient fusion graphs.",
            "Overly aggressive fusion may generate kernels exceeding on\u2011chip instruction\u2011cache limits on some MCUs, leading to fallback penalties.",
            "Generalization to unseen device states (e.g., sudden thermal throttling) is limited; periodic re\u2011training or online adaptation may be necessary.",
            "Debugging dynamically generated fusion graphs can be more complex than static graph deployment, especially on resource\u2011constrained environments."
        ]
    }
]