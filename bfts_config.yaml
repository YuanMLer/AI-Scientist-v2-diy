# =============================================================================
# Part 1: Core Configuration (Application Logic, Data, Experiments)
# =============================================================================

# path to the task data directory
data_dir: "data"
preprocess_data: False

goal: null
eval: null

log_dir: logs
workspace_dir: workspaces

# whether to copy the data to the workspace directory (otherwise it will be symlinked)
# copying is recommended to prevent the agent from accidentally modifying the original data
copy_data: True

exp_name: run # a random experiment name will be generated if not provided

# settings for code execution
exec:
  timeout: 3600
  agent_file_name: runfile.py
  format_tb_ipython: False

generate_report: True
# LLM settings for final report from journal
# Referenced from Global LLM Configuration
report: ${llm_config.profiles.report}

experiment:
  num_syn_datasets: 1

debug:
  stage4: False

# agent hyperparams
agent:
  type: parallel
  num_workers: 4
  stages:
    stage1_max_iters: 20
    stage2_max_iters: 12
    stage3_max_iters: 12
    stage4_max_iters: 18
  # how many improvement iterations to run
  steps: 5 # if stage-specific max_iters are not provided, the agent will use this value for all stages
  # whether to instruct the agent to use CV (set to 1 to disable)
  k_fold_validation: 1
  multi_seed_eval:
    num_seeds: 3 # should be the same as num_workers if num_workers < 3. Otherwise, set it to be 3.
  # whether to instruct the agent to generate a prediction function
  expose_prediction: False
  # whether to provide the agent with a preview of the data
  data_preview: False

  # LLM settings for coding
  # Referenced from Global LLM Configuration
  code: ${llm_config.profiles.code}

  # LLM settings for evaluating program output / tracebacks
  # Referenced from Global LLM Configuration
  feedback: ${llm_config.profiles.feedback}

  # VLM settings
  # Referenced from Global LLM Configuration
  vlm_feedback: ${llm_config.profiles.vlm_feedback}

  summary: ${llm_config.profiles.summary}

  search:
    max_debug_depth: 3
    debug_prob: 0.5
    num_drafts: 3



# =============================================================================
# Part 2: Global LLM Configuration
# =============================================================================
# This section centralizes all Large Language Model (LLM) configurations.
# It includes:
# 1. Model definitions (Providers, API Keys, Base URLs)
# 2. Usage profiles (Task-specific settings like temperature, tokens)

llm_config:
  default_model: "ollama/nemotron-3-nano:latest"
  
  # Global timeout for LLM calls (can be overridden per model)
  timeout: 120

  # Default configurations for providers to allow dynamic model names (e.g. ollama/new-model)
  defaults:
    ollama:
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"
  
  # -------------------------------------------------------------------------
  # Usage Profiles (Task-specific configurations)
  # -------------------------------------------------------------------------
  profiles:
    # Profile for generating code (coding agent)
    code:
      model: "ollama/nemotron-3-nano:latest"
      temp: 1.0
      max_tokens: 12000
    
    # Profile for providing feedback and evaluation
    feedback:
      model: "ollama/nemotron-3-nano:latest"
      temp: 0.5
      max_tokens: 8192
    
    # Profile for VLM (Visual Language Model) feedback
    vlm_feedback:
      model: "ollama/nemotron-3-nano:latest"
      temp: 0.5
      max_tokens: null
    
    # Profile for generating the final report
    report:
      model: "ollama/nemotron-3-nano:latest"
      temp: 1.0
      max_tokens: 8192
    
    # Profile for summarizing the experiment
    summary:
      model: "ollama/nemotron-3-nano:latest"
      temp: 0.5
      max_tokens: 8192

  # -------------------------------------------------------------------------
  # Model Definitions
  # -------------------------------------------------------------------------
  models:
    # Ollama Models
    "ollama/nemotron-3-nano:latest":
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"
    
    "ollama/gpt-oss:20b":
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"
      
    "ollama/qwen3:8b":
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"

    # Openrouter Models
    "anthropic/claude-haiku-4.5":
      provider: "openai"
      base_url: "${oc.env:OPENAI_BASE_URL, 'https://openrouter.ai/api/v1/'}"
      api_key: "${oc.env:OPENROUTER_API_KEY, ''}"
      
    "anthropic/claude-opus-4.5":
      provider: "openai"
      base_url: "${oc.env:OPENAI_BASE_URL, 'https://openrouter.ai/api/v1/'}"
      api_key: "${oc.env:OPENROUTER_API_KEY, ''}"
      
    "anthropic/claude-sonnet-4.5":
      provider: "openai"
      base_url: "${oc.env:OPENAI_BASE_URL, 'https://openrouter.ai/api/v1/'}"
      api_key: "${oc.env:OPENROUTER_API_KEY, ''}"
      
    "openai/gpt-5":
      provider: "openai"
      base_url: "${oc.env:ANTHROPIC_BASE_URL, 'https://openrouter.ai/api/v1/'}"
      api_key: "${oc.env:ANTHROPIC_API_KEY, ''}"
      
    
    # VLM Models (Ollama)
    "ollama/llama4:16x17b":
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"
      
    "ollama/mistral-small3.2:24b":
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"
      
    "ollama/qwen2.5vl:32b":
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"
      
    "ollama/z-uo/qwen2.5vl_tools:32b":
      provider: "ollama"
      base_url: "${oc.env:OLLAMA_BASE_URL, 'http://1.13.248.121:15699/v1'}"
      api_key: "${oc.env:OLLAMA_API_KEY, 'ollama'}"
